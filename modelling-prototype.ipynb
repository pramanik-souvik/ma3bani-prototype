{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a41b9c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USERAS\\anaconda3\\envs\\resPy\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21000000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 9000000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"data/data-train-prototype.csv\",\n",
    "        \"test\": \"data/data-test-prototype.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04ed1f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hugging Face tokenizer created and saved.\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models\n",
    "from tokenizers.normalizers import NFKC, Sequence as NormalizerSequence\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Step 1: Create and train tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = NormalizerSequence([NFKC()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\", \"<mask>\"]\n",
    ")\n",
    "\n",
    "tokenizer.train(files=[\"ma3bani-prototype-corpus.txt\"], trainer=trainer)\n",
    "\n",
    "# Step 2: Save to Hugging Face-compatible tokenizer\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    mask_token=\"<mask>\"\n",
    ")\n",
    "\n",
    "hf_tokenizer.save_pretrained(\"ma3bani-prototype-tokenizer\")\n",
    "\n",
    "print(\"âœ… Hugging Face tokenizer created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f8e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizer successfully trained and saved to 'ma3bani-tokenizer/'\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.normalizers import Sequence, NFKC\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Step 1: Initialize tokenizer\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "\n",
    "# Step 2: Normalize and pre-tokenize\n",
    "tokenizer.normalizer = Sequence([NFKC()])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Step 3: Set training parameters\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\", \"<mask>\"]\n",
    ")\n",
    "\n",
    "# Step 4: Train tokenizer\n",
    "tokenizer.train([\"ma3bani-prototype-corpus.txt\"], trainer)\n",
    "\n",
    "# Step 5: Wrap with Hugging Face and save\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    mask_token=\"<mask>\"\n",
    ")\n",
    "\n",
    "hf_tokenizer.save_pretrained(\"ma3bani-prototype-tokenizer\")\n",
    "\n",
    "print(\"âœ… Tokenizer successfully trained and saved to 'ma3bani-tokenizer/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0313653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ma3bani-prototype-tokenizer\\\\tokenizer_config.json',\n",
       " 'ma3bani-prototype-tokenizer\\\\special_tokens_map.json',\n",
       " 'ma3bani-prototype-tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"ma3bani-prototype-tokenizer\")\n",
    "tokenizer.save_pretrained(\"ma3bani-prototype-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc1d65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, DataCollatorForLanguageModeling\n",
    "from functools import partial\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"ma3bani-prototype-tokenizer\")\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=256, padding=\"max_length\")\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    partial(tokenize_function, tokenizer=tokenizer),\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9db36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=384, #500\n",
    "    num_hidden_layers=4, #6\n",
    "    num_attention_heads=8, #16,12\n",
    "    intermediate_size=2048,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USERAS\\AppData\\Local\\Temp\\ipykernel_62316\\1274821384.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230002' max='656250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230002/656250 7:54:20 < 14:39:04, 8.08 it/s, Epoch 0.70/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>7.481100</td>\n",
       "      <td>7.626669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>7.568500</td>\n",
       "      <td>7.447351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>7.434900</td>\n",
       "      <td>7.236320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>7.324200</td>\n",
       "      <td>7.141355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>7.205500</td>\n",
       "      <td>7.113891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>7.091000</td>\n",
       "      <td>6.912715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>7.000600</td>\n",
       "      <td>6.674437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>6.885700</td>\n",
       "      <td>6.641129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>6.777800</td>\n",
       "      <td>6.531026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>6.682600</td>\n",
       "      <td>6.435897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>6.611400</td>\n",
       "      <td>6.436038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>6.542300</td>\n",
       "      <td>6.435234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>6.451600</td>\n",
       "      <td>6.367721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>6.418400</td>\n",
       "      <td>6.312330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>6.354400</td>\n",
       "      <td>6.030913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>6.320800</td>\n",
       "      <td>6.259633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>6.282200</td>\n",
       "      <td>6.108234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>6.222800</td>\n",
       "      <td>6.130497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>6.196300</td>\n",
       "      <td>5.936563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>6.160500</td>\n",
       "      <td>6.026937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>6.114800</td>\n",
       "      <td>5.885516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>6.086000</td>\n",
       "      <td>5.883867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>6.057100</td>\n",
       "      <td>5.949251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>6.034000</td>\n",
       "      <td>5.895105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>6.021300</td>\n",
       "      <td>5.877301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>5.966200</td>\n",
       "      <td>5.795718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>5.961200</td>\n",
       "      <td>5.820515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>5.926300</td>\n",
       "      <td>5.708213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>5.902400</td>\n",
       "      <td>5.751665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>5.877100</td>\n",
       "      <td>5.701816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>5.850400</td>\n",
       "      <td>5.868828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>5.840000</td>\n",
       "      <td>5.719099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>5.835700</td>\n",
       "      <td>5.753655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>5.790600</td>\n",
       "      <td>5.706328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>5.781400</td>\n",
       "      <td>5.564115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>5.752700</td>\n",
       "      <td>5.578271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>5.749100</td>\n",
       "      <td>5.499454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>5.720000</td>\n",
       "      <td>5.471291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>5.704700</td>\n",
       "      <td>5.597762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>5.697800</td>\n",
       "      <td>5.696095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>5.667900</td>\n",
       "      <td>5.576755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>5.653100</td>\n",
       "      <td>5.512020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>5.646300</td>\n",
       "      <td>5.388689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>5.631300</td>\n",
       "      <td>5.519336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>5.623500</td>\n",
       "      <td>5.486104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>5.599900</td>\n",
       "      <td>5.464059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>5.599400</td>\n",
       "      <td>5.414055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>5.577100</td>\n",
       "      <td>5.524311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>5.587700</td>\n",
       "      <td>5.548306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>5.570800</td>\n",
       "      <td>5.451342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>5.542300</td>\n",
       "      <td>5.520130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>5.534200</td>\n",
       "      <td>5.465089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106000</td>\n",
       "      <td>5.519000</td>\n",
       "      <td>5.406017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>5.505500</td>\n",
       "      <td>5.429546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>5.498000</td>\n",
       "      <td>5.416322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>5.484800</td>\n",
       "      <td>5.412550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>5.479100</td>\n",
       "      <td>5.241870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>5.459000</td>\n",
       "      <td>5.366455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118000</td>\n",
       "      <td>5.458500</td>\n",
       "      <td>5.270754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>5.440600</td>\n",
       "      <td>5.273523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122000</td>\n",
       "      <td>5.445300</td>\n",
       "      <td>5.351412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>5.427600</td>\n",
       "      <td>5.269024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>5.423100</td>\n",
       "      <td>5.227983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>5.405100</td>\n",
       "      <td>5.460176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>5.401800</td>\n",
       "      <td>5.108915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>5.392200</td>\n",
       "      <td>5.233103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134000</td>\n",
       "      <td>5.377300</td>\n",
       "      <td>5.230727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>5.375000</td>\n",
       "      <td>5.288070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>5.347200</td>\n",
       "      <td>5.189885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>5.345400</td>\n",
       "      <td>5.180827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142000</td>\n",
       "      <td>5.328000</td>\n",
       "      <td>5.136065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>5.352200</td>\n",
       "      <td>5.240765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146000</td>\n",
       "      <td>5.328300</td>\n",
       "      <td>5.229706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>5.317900</td>\n",
       "      <td>5.217596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>5.324800</td>\n",
       "      <td>5.290832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>5.289400</td>\n",
       "      <td>5.308973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154000</td>\n",
       "      <td>5.284000</td>\n",
       "      <td>5.059496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>5.282500</td>\n",
       "      <td>5.142425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158000</td>\n",
       "      <td>5.270100</td>\n",
       "      <td>5.180954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>5.275200</td>\n",
       "      <td>5.181149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>5.265000</td>\n",
       "      <td>5.111269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>5.264500</td>\n",
       "      <td>5.139715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166000</td>\n",
       "      <td>5.262800</td>\n",
       "      <td>5.117887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>5.256000</td>\n",
       "      <td>5.150054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>5.246000</td>\n",
       "      <td>5.114875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>5.235500</td>\n",
       "      <td>5.118630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>5.214900</td>\n",
       "      <td>5.096203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176000</td>\n",
       "      <td>5.216800</td>\n",
       "      <td>4.968451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178000</td>\n",
       "      <td>5.221400</td>\n",
       "      <td>4.996745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>5.217500</td>\n",
       "      <td>5.031262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182000</td>\n",
       "      <td>5.196200</td>\n",
       "      <td>5.114174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184000</td>\n",
       "      <td>5.204500</td>\n",
       "      <td>5.099185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186000</td>\n",
       "      <td>5.206900</td>\n",
       "      <td>4.952697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188000</td>\n",
       "      <td>5.183600</td>\n",
       "      <td>4.937121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>5.172300</td>\n",
       "      <td>5.042632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192000</td>\n",
       "      <td>5.176500</td>\n",
       "      <td>4.939270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194000</td>\n",
       "      <td>5.164400</td>\n",
       "      <td>5.023302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196000</td>\n",
       "      <td>5.169100</td>\n",
       "      <td>4.944789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198000</td>\n",
       "      <td>5.153400</td>\n",
       "      <td>5.085963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>5.160500</td>\n",
       "      <td>4.949843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202000</td>\n",
       "      <td>5.138200</td>\n",
       "      <td>5.060310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204000</td>\n",
       "      <td>5.128000</td>\n",
       "      <td>5.099736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206000</td>\n",
       "      <td>5.128200</td>\n",
       "      <td>5.053489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208000</td>\n",
       "      <td>5.113100</td>\n",
       "      <td>4.884796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>5.106900</td>\n",
       "      <td>4.988850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212000</td>\n",
       "      <td>5.103500</td>\n",
       "      <td>4.891356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214000</td>\n",
       "      <td>5.106700</td>\n",
       "      <td>4.953649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216000</td>\n",
       "      <td>5.107700</td>\n",
       "      <td>5.026956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218000</td>\n",
       "      <td>5.103200</td>\n",
       "      <td>4.925645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>5.093900</td>\n",
       "      <td>4.950795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222000</td>\n",
       "      <td>5.094700</td>\n",
       "      <td>4.849225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224000</td>\n",
       "      <td>5.089400</td>\n",
       "      <td>4.966304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226000</td>\n",
       "      <td>5.095300</td>\n",
       "      <td>4.927306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228000</td>\n",
       "      <td>5.075300</td>\n",
       "      <td>4.872005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>5.077400</td>\n",
       "      <td>4.902838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import math\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     loss =eval_pred.metrics[\"eval_loss\"]                                            #Saving for later\n",
    "#     return {\"perplexity\": math.exp(loss)} if loss else {}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ma3bani-prototype\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_accumulation_steps=8,\n",
    "    # predict_with_generate=False,\n",
    "    logging_steps=1000,\n",
    "    logging_dir='./logs',\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"].select(range(1000)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
